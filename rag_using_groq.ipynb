{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627cd698",
   "metadata": {},
   "source": [
    "# ==============================================================================\n",
    "# Groq RAG Pipeline\n",
    "#\n",
    "# Description:\n",
    "# This script implements a complete Retrieval-Augmented Generation (RAG) pipeline.\n",
    "# It uses the Groq API for ultra-fast language model inference and a local,\n",
    "# open-source SentenceTransformer model for creating text embeddings.\n",
    "#\n",
    "# Pipeline Steps:\n",
    "# 1.  Load Data: Extracts text from a specified PDF file.\n",
    "# 2.  Chunk Data: Splits the extracted text into smaller, overlapping chunks.\n",
    "# 3.  Embed Chunks: Converts text chunks into numerical vectors (embeddings).\n",
    "# 4.  Semantic Search: Finds the most relevant chunks based on a user query.\n",
    "# 5.  Generate Response: Sends the query and relevant chunks to a Groq LLM\n",
    "#     to generate a context-aware answer.\n",
    "# 6.  Evaluate Response: Uses a second Groq LLM call to score the answer\n",
    "#     against a predefined ideal response.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c97b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JVAI-Practice\\simple-rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Environment Setup and Library Imports ---\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF library\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def initialize_clients():\n",
    "    \"\"\"Loads environment variables and initializes API clients and models.\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Initialize Groq Client for LLM inference\n",
    "    try:\n",
    "        groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "        if not groq_api_key:\n",
    "            raise ValueError(\"GROQ_API_KEY not found in the .env file.\")\n",
    "        client = Groq(api_key=groq_api_key)\n",
    "        print(\"âœ… Groq client initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ Error initializing Groq client: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    # Load a local SentenceTransformer model for creating embeddings\n",
    "    try:\n",
    "        # 'all-MiniLM-L6-v2' is a great default for its balance of speed and performance.\n",
    "        model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        print(\"âœ… SentenceTransformer embedding model loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ Error loading embedding model: {e}\")\n",
    "        return client, None\n",
    "        \n",
    "    return client, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aa80c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Data Processing Functions ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str | None:\n",
    "    \"\"\"Extracts all text content from a PDF file.\"\"\"\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"ðŸ”¥ Error: PDF file not found at '{pdf_path}'\")\n",
    "        return None\n",
    "    try:\n",
    "        doc = fitz.open(pdf_path)\n",
    "        all_text = \"\".join(page.get_text() for page in doc)\n",
    "        doc.close()\n",
    "        return all_text\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ Error reading PDF file: {e}\")\n",
    "        return None\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> list[str]:\n",
    "    \"\"\"Splits a long text into smaller, overlapping chunks.\"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size - overlap)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6367580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Embedding and Search Functions ---\n",
    "\n",
    "def create_embeddings(chunks: list[str], embedding_model) -> np.ndarray | None:\n",
    "    \"\"\"Creates numerical vector embeddings for a list of text chunks.\"\"\"\n",
    "    if not chunks or embedding_model is None:\n",
    "        return None\n",
    "    try:\n",
    "        embeddings = embedding_model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ Error creating embeddings: {e}\")\n",
    "        return None\n",
    "\n",
    "def semantic_search(query: str, chunks: list[str], embeddings: np.ndarray, embedding_model, k: int = 3) -> list[str]:\n",
    "    \"\"\"Finds the top 'k' most relevant chunks for a given query.\"\"\"\n",
    "    if embeddings is None or embedding_model is None:\n",
    "        return []\n",
    "    \n",
    "    # 1. Create an embedding for the user's query.\n",
    "    query_embedding = embedding_model.encode([query], convert_to_numpy=True)[0]\n",
    "    \n",
    "    # 2. Calculate cosine similarity between the query and all chunk embeddings.\n",
    "    # The formula is: (A â‹… B) / (||A|| * ||B||)\n",
    "    dot_products = np.dot(embeddings, query_embedding)\n",
    "    norm_products = np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "    similarities = dot_products / norm_products\n",
    "    \n",
    "    # 3. Get the indices of the top 'k' most similar chunks.\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    \n",
    "    # 4. Return the corresponding chunks.\n",
    "    return [chunks[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13d67bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. LLM Generation and Evaluation Functions ---\n",
    "\n",
    "def generate_groq_response(client, system_prompt: str, user_message: str, model: str = \"llama3-8b-8192\") -> str:\n",
    "    \"\"\"Generates a response from the Groq API based on a prompt.\"\"\"\n",
    "    if client is None:\n",
    "        return \"ðŸ”¥ Error: Groq client is not initialized.\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            temperature=0.2,  # Lower temperature for more factual, deterministic answers\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message},\n",
    "            ],\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"ðŸ”¥ Error generating response from Groq: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b99aa31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ðŸš€ Starting RAG Pipeline with Groq and Local Embeddings ---\n",
      "âœ… Groq client initialized successfully.\n",
      "âœ… SentenceTransformer embedding model loaded successfully.\n",
      "ðŸ“„ Text from 'data/AI_information.pdf' extracted and split into 42 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Embeddings created for all chunks with shape: (42, 384)\n",
      "\n",
      "â“ Loaded Query: What is 'Explainable AI' and why is it considered important?\n",
      "âœ… Semantic search complete. Retrieved top relevant context chunks.\n",
      "\n",
      "ðŸ’¬ Generating response from Groq based on retrieved context...\n",
      "\n",
      "ðŸ’¡ AI Response:\n",
      " According to the provided context, Explainable AI (XAI) is a technique aimed at making AI decisions more understandable, enabling users to assess their fairness and accuracy. It is considered important because it is essential for building trust in AI systems.\n",
      "\n",
      "\n",
      "--- âš–ï¸ Evaluating AI Response ---\n",
      "\n",
      "â­ Evaluation Result:\n",
      " Score: 0.5\n",
      "\n",
      "Justification: The AI response is partially correct. It correctly identifies the goal of Explainable AI (XAI) as making AI decisions more understandable and enabling users to assess their fairness and accuracy. However, it does not mention the importance of XAI for building trust, accountability, and ensuring fairness in AI systems, which is a crucial aspect of the true response.\n",
      "\n",
      "--- âœ… Pipeline Finished Successfully ---\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Main Pipeline Execution ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the entire RAG pipeline.\"\"\"\n",
    "    print(\"--- ðŸš€ Starting RAG Pipeline with Groq and Local Embeddings ---\")\n",
    "    \n",
    "    # Initialize clients and models\n",
    "    groq_client, embedding_model = initialize_clients()\n",
    "    if not groq_client or not embedding_model:\n",
    "        print(\"--- ðŸ›‘ Pipeline halted due to initialization errors. ---\")\n",
    "        return\n",
    "\n",
    "    # Part 1: Data Ingestion and Chunking\n",
    "    pdf_path = \"data/AI_information.pdf\"\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    if not extracted_text:\n",
    "        print(\"--- ðŸ›‘ Pipeline halted because text could not be extracted. ---\")\n",
    "        return\n",
    "        \n",
    "    text_chunks = chunk_text(extracted_text)\n",
    "    print(f\"ðŸ“„ Text from '{pdf_path}' extracted and split into {len(text_chunks)} chunks.\")\n",
    "\n",
    "    # Part 2: Embedding Creation\n",
    "    chunk_embeddings = create_embeddings(text_chunks, embedding_model)\n",
    "    if chunk_embeddings is None:\n",
    "        print(\"--- ðŸ›‘ Pipeline halted because embeddings could not be created. ---\")\n",
    "        return\n",
    "    print(f\"âœ… Embeddings created for all chunks with shape: {chunk_embeddings.shape}\")\n",
    "\n",
    "    # Part 3: Load Query and Perform Semantic Search\n",
    "    try:\n",
    "        with open('data/val.json') as f:\n",
    "            val_data = json.load(f)[0]\n",
    "        query = val_data['question']\n",
    "        ideal_answer = val_data['ideal_answer']\n",
    "        print(f\"\\nâ“ Loaded Query: {query}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ðŸ”¥ Could not load 'data/val.json': {e}. Using a default query.\")\n",
    "        query = \"What is Explainable AI and why is it important?\"\n",
    "        ideal_answer = \"Explainable AI (XAI) makes AI decisions understandable to humans, which is crucial for trust, fairness, and accountability.\"\n",
    "\n",
    "    top_chunks = semantic_search(query, text_chunks, chunk_embeddings, embedding_model, k=3)\n",
    "    print(\"âœ… Semantic search complete. Retrieved top relevant context chunks.\")\n",
    "\n",
    "    # Part 4: Generate Response using RAG\n",
    "    print(\"\\nðŸ’¬ Generating response from Groq based on retrieved context...\")\n",
    "    rag_system_prompt = \"You are a helpful AI assistant. Answer the user's question based ONLY on the provided context. If the context does not contain the answer, state that clearly.\"\n",
    "    context_str = \"\\n\\n---\\n\\n\".join(top_chunks)\n",
    "    user_prompt_for_rag = f\"Context:\\n{context_str}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    ai_response = generate_groq_response(groq_client, rag_system_prompt, user_prompt_for_rag)\n",
    "    print(\"\\nðŸ’¡ AI Response:\\n\", ai_response)\n",
    "\n",
    "    # Part 5: Evaluate the Generated Response\n",
    "    print(\"\\n\\n--- âš–ï¸ Evaluating AI Response ---\")\n",
    "    eval_system_prompt = \"You are an expert evaluation system. Compare the 'AI Response' to the 'True Response' based on the user's query. Score the AI response on a scale of 0, 0.5, or 1. '1' for a correct and complete answer, '0.5' for a partially correct answer, and '0' for an incorrect answer. Provide only the score and a brief justification.\"\n",
    "    eval_user_prompt = f\"User Query: {query}\\n\\nTrue Response: {ideal_answer}\\n\\nAI Response: {ai_response}\"\n",
    "    \n",
    "    evaluation_response = generate_groq_response(groq_client, eval_system_prompt, eval_user_prompt)\n",
    "    print(\"\\nâ­ Evaluation Result:\\n\", evaluation_response)\n",
    "    \n",
    "    print(\"\\n--- âœ… Pipeline Finished Successfully ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
